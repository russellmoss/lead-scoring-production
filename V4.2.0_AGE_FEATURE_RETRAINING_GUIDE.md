# V4.2.0 Age Feature Retraining Guide

**Version**: V4.2.0_AGE_BUCKET  
**Created**: January 7, 2026  
**Purpose**: Add age_bucket feature to V4 model and validate improvement  
**Status**: ğŸ”¬ Ready for Execution

---

## Executive Summary

### What We're Doing
Adding `age_bucket` as the 23rd feature to V4.1.0 (currently 22 features) based on analysis showing:
- Age correlation with `experience_years` = **0.072** (not redundant)
- V4.1.0 doesn't fully capture age signal (65+ converts at 1.22% vs 3.46% for under 50)
- Even V4 top decile shows age gap (3.58% vs 4.83%)

### Critical Validation Gates

| Gate | Criterion | Action if FAIL |
|------|-----------|----------------|
| **G1** | V4.2.0 Test AUC â‰¥ V4.1.0 Test AUC (0.620) | âŒ ABORT - Do not deploy |
| **G2** | V4.2.0 Top Decile Lift â‰¥ V4.1.0 (2.03x) | âŒ ABORT - Do not deploy |
| **G3** | Overfitting gap < 0.15 (Train AUC - Test AUC) | âŒ ABORT - Retune hyperparameters |
| **G4** | Age feature importance > 0 | âš ï¸ WARNING - Feature may be useless |

**If ANY gate fails, we DO NOT deploy V4.2.0.**

---

## Pre-Flight Checklist

Before starting, verify:

- [ ] BigQuery MCP connected
- [ ] Access to `savvy-gtm-analytics` project
- [ ] Python environment with: `xgboost`, `shap`, `pandas`, `scikit-learn`, `google-cloud-bigquery`
- [ ] Write access to `ml_features` dataset
- [ ] Local directory structure exists:
  ```
  v4/
  â”œâ”€â”€ models/
  â”‚   â””â”€â”€ v4.2.0/  (create this)
  â”œâ”€â”€ reports/
  â”‚   â””â”€â”€ v4.2/    (create this)
  â””â”€â”€ sql/
  ```

---

## Phase 1: Feature Engineering SQL Updates

### Step 1.1: Update `v4_prospect_features` View

Add age_bucket to the production features view.

```sql
-- ============================================================================
-- V4.2.0: UPDATE v4_prospect_features VIEW WITH AGE_BUCKET
-- ============================================================================
-- Location: ml_features.v4_prospect_features
-- Change: Add age_bucket as 23rd feature
-- ============================================================================

CREATE OR REPLACE VIEW `savvy-gtm-analytics.ml_features.v4_prospect_features` AS

WITH base_features AS (
    -- [EXISTING V4.1.0 FEATURE LOGIC - KEEP ALL 22 FEATURES]
    SELECT 
        crd,
        -- ... all existing features ...
        tenure_months,
        tenure_bucket_encoded,
        experience_years,
        mobility_3yr,
        mobility_tier_encoded,
        firm_net_change_12mo,
        firm_stability_tier_encoded,
        firm_rep_count_at_contact,
        is_wirehouse,
        is_broker_protocol,
        has_email,
        has_linkedin,
        has_firm_data,
        short_tenure_x_high_mobility,
        mobility_x_heavy_bleeding,
        is_recent_mover,
        days_since_last_move,
        firm_departures_corrected,
        bleeding_velocity_encoded,
        is_independent_ria,
        is_ia_rep_type,
        is_dual_registered
    FROM `savvy-gtm-analytics.ml_features.v4_production_features_v41`
),

-- NEW: Add age bucket from FINTRX
age_data AS (
    SELECT 
        RIA_CONTACT_CRD_ID as crd,
        AGE_RANGE,
        -- V4.2.0: Age bucket encoding (based on age_analysis_results.md)
        CASE 
            WHEN AGE_RANGE IN ('18-24', '25-29', '30-34') THEN 0      -- UNDER_35 (4.06% conv)
            WHEN AGE_RANGE IN ('35-39', '40-44', '45-49') THEN 1      -- 35_49 (3.82% conv)
            WHEN AGE_RANGE IN ('50-54', '55-59', '60-64') THEN 2      -- 50_64 (3.56% conv)
            WHEN AGE_RANGE IN ('65-69') THEN 3                         -- 65_69 (2.97% conv)
            WHEN AGE_RANGE IN ('70-74', '75-79', '80-84', '85-89', '90-94', '95-99') THEN 4  -- 70_PLUS (1.48% conv)
            ELSE 2  -- UNKNOWN defaults to median (50_64)
        END as age_bucket_encoded
    FROM `savvy-gtm-analytics.FinTrx_data_CA.ria_contacts_current`
)

SELECT 
    bf.*,
    -- V4.2.0: NEW FEATURE
    COALESCE(ad.age_bucket_encoded, 2) as age_bucket_encoded  -- Default to median if missing
FROM base_features bf
LEFT JOIN age_data ad ON bf.crd = ad.crd;
```

**Verification Query:**
```sql
-- Verify age_bucket distribution in prospect features
SELECT 
    age_bucket_encoded,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct
FROM `savvy-gtm-analytics.ml_features.v4_prospect_features`
GROUP BY age_bucket_encoded
ORDER BY age_bucket_encoded;
```

**Expected Output:**
| age_bucket_encoded | count | pct |
|--------------------|-------|-----|
| 0 (UNDER_35) | ~15% | |
| 1 (35_49) | ~35% | |
| 2 (50_64) | ~30% | |
| 3 (65_69) | ~10% | |
| 4 (70_PLUS) | ~5% | |

---

### Step 1.2: Update Training Features Table

Add age_bucket to the PIT training features.

```sql
-- ============================================================================
-- V4.2.0: UPDATE v4_features_pit TABLE WITH AGE_BUCKET
-- ============================================================================
-- This creates the training dataset with age_bucket
-- NOTE: We use current AGE_RANGE as proxy (age buckets are 5-year ranges, 
--       so historical leads are likely still in same bucket or +1)
-- ============================================================================

CREATE OR REPLACE TABLE `savvy-gtm-analytics.ml_features.v4_features_pit_v42` AS

WITH existing_features AS (
    SELECT *
    FROM `savvy-gtm-analytics.ml_features.v4_features_pit_v41`
),

age_data AS (
    SELECT 
        RIA_CONTACT_CRD_ID as crd,
        CASE 
            WHEN AGE_RANGE IN ('18-24', '25-29', '30-34') THEN 0
            WHEN AGE_RANGE IN ('35-39', '40-44', '45-49') THEN 1
            WHEN AGE_RANGE IN ('50-54', '55-59', '60-64') THEN 2
            WHEN AGE_RANGE IN ('65-69') THEN 3
            WHEN AGE_RANGE IN ('70-74', '75-79', '80-84', '85-89', '90-94', '95-99') THEN 4
            ELSE 2
        END as age_bucket_encoded
    FROM `savvy-gtm-analytics.FinTrx_data_CA.ria_contacts_current`
)

SELECT 
    ef.*,
    COALESCE(ad.age_bucket_encoded, 2) as age_bucket_encoded
FROM existing_features ef
LEFT JOIN age_data ad ON ef.advisor_crd = ad.crd;
```

**Verification Query:**
```sql
-- Verify training data has age_bucket
SELECT 
    split,
    COUNT(*) as total,
    AVG(age_bucket_encoded) as avg_age_bucket,
    COUNTIF(age_bucket_encoded IS NULL) as null_count
FROM `savvy-gtm-analytics.ml_features.v4_features_pit_v42` f
JOIN `savvy-gtm-analytics.ml_features.v4_splits_v41` s 
    ON f.lead_id = s.lead_id
GROUP BY split;
```

---

## Phase 2: Model Training

### Step 2.1: Python Training Script

Create/update the training script with age_bucket feature.

```python
"""
V4.2.0 Model Training Script - Age Bucket Feature Addition
==========================================================
Run this script to train V4.2.0 with age_bucket as 23rd feature.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import roc_auc_score, average_precision_score
import shap
import pickle
import json
from datetime import datetime
from google.cloud import bigquery
import os

# Configuration
MODEL_VERSION = "v4.2.0"
OUTPUT_DIR = f"v4/models/{MODEL_VERSION}"
REPORTS_DIR = "v4/reports/v4.2"

# Create directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(REPORTS_DIR, exist_ok=True)

# V4.2.0 Features (23 total - was 22 in V4.1.0)
FEATURES_V42 = [
    # Original V4 features (12)
    'tenure_months',
    'tenure_bucket_encoded',
    'experience_years',
    'mobility_3yr',
    'mobility_tier_encoded',
    'firm_net_change_12mo',
    'firm_stability_tier_encoded',
    'firm_rep_count_at_contact',
    'is_wirehouse',
    'is_broker_protocol',
    'has_email',
    'has_linkedin',
    # V4.1 Bleeding features (4)
    'is_recent_mover',
    'days_since_last_move',
    'firm_departures_corrected',
    'bleeding_velocity_encoded',
    # V4.1 Firm/Rep type features (3)
    'is_independent_ria',
    'is_ia_rep_type',
    'is_dual_registered',
    # Interaction features (2)
    'short_tenure_x_high_mobility',
    'mobility_x_heavy_bleeding',
    # V4.2.0 NEW: Age feature (1)
    'age_bucket_encoded'  # NEW!
]

# Hyperparameters (same as V4.1.0 - may need tuning)
HYPERPARAMETERS = {
    "max_depth": 2,
    "min_child_weight": 30,
    "reg_alpha": 1.0,
    "reg_lambda": 5.0,
    "gamma": 0.3,
    "learning_rate": 0.01,
    "n_estimators": 2000,
    "early_stopping_rounds": 150,
    "subsample": 0.6,
    "colsample_bytree": 0.6,
    "base_score": 0.5,
    "scale_pos_weight": 41.0,  # Class imbalance ratio
    "objective": "binary:logistic",
    "eval_metric": "auc",
    "random_state": 42
}

def load_training_data():
    """Load training data from BigQuery."""
    client = bigquery.Client(project='savvy-gtm-analytics')
    
    query = """
    SELECT 
        f.*,
        s.split,
        CASE WHEN f.target = 1 THEN 1 ELSE 0 END as target_binary
    FROM `savvy-gtm-analytics.ml_features.v4_features_pit_v42` f
    JOIN `savvy-gtm-analytics.ml_features.v4_splits_v41` s 
        ON f.lead_id = s.lead_id
    WHERE s.split IN ('TRAIN', 'TEST')
    """
    
    df = client.query(query).to_dataframe()
    print(f"Loaded {len(df)} rows")
    print(f"Train: {len(df[df['split'] == 'TRAIN'])}, Test: {len(df[df['split'] == 'TEST'])}")
    return df


def train_model(df):
    """Train XGBoost model with V4.2.0 features."""
    
    # Split data
    train_df = df[df['split'] == 'TRAIN'].copy()
    test_df = df[df['split'] == 'TEST'].copy()
    
    X_train = train_df[FEATURES_V42]
    y_train = train_df['target_binary']
    X_test = test_df[FEATURES_V42]
    y_test = test_df['target_binary']
    
    print(f"\nFeature count: {len(FEATURES_V42)}")
    print(f"Training samples: {len(X_train)}, positives: {y_train.sum()}")
    print(f"Test samples: {len(X_test)}, positives: {y_test.sum()}")
    
    # Check for missing age_bucket values
    print(f"\nAge bucket distribution (train):")
    print(X_train['age_bucket_encoded'].value_counts().sort_index())
    
    # Train XGBoost
    model = xgb.XGBClassifier(**HYPERPARAMETERS)
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=100
    )
    
    # Predictions
    y_train_pred = model.predict_proba(X_train)[:, 1]
    y_test_pred = model.predict_proba(X_test)[:, 1]
    
    # Metrics
    train_auc = roc_auc_score(y_train, y_train_pred)
    test_auc = roc_auc_score(y_test, y_test_pred)
    test_ap = average_precision_score(y_test, y_test_pred)
    
    print(f"\n{'='*50}")
    print(f"V4.2.0 TRAINING RESULTS")
    print(f"{'='*50}")
    print(f"Train AUC-ROC: {train_auc:.4f}")
    print(f"Test AUC-ROC:  {test_auc:.4f}")
    print(f"Test AUC-PR:   {test_ap:.4f}")
    print(f"Overfitting Gap: {train_auc - test_auc:.4f}")
    print(f"{'='*50}")
    
    return model, {
        'train_auc': train_auc,
        'test_auc': test_auc,
        'test_ap': test_ap,
        'overfitting_gap': train_auc - test_auc,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'y_test_pred': y_test_pred
    }


def validate_gates(metrics):
    """
    CRITICAL: Validate improvement gates.
    Returns (passed, gate_results)
    """
    V41_TEST_AUC = 0.620
    V41_TOP_DECILE_LIFT = 2.03
    MAX_OVERFIT_GAP = 0.15
    
    gates = {}
    
    # Gate 1: Test AUC >= V4.1.0
    gates['G1_AUC'] = {
        'criterion': f'Test AUC >= {V41_TEST_AUC}',
        'actual': metrics['test_auc'],
        'passed': metrics['test_auc'] >= V41_TEST_AUC,
        'improvement': f"+{(metrics['test_auc'] - V41_TEST_AUC)*100:.2f}%" if metrics['test_auc'] >= V41_TEST_AUC else f"{(metrics['test_auc'] - V41_TEST_AUC)*100:.2f}%"
    }
    
    # Gate 3: Overfitting check
    gates['G3_OVERFIT'] = {
        'criterion': f'Overfitting gap < {MAX_OVERFIT_GAP}',
        'actual': metrics['overfitting_gap'],
        'passed': metrics['overfitting_gap'] < MAX_OVERFIT_GAP
    }
    
    # Print results
    print(f"\n{'='*50}")
    print("VALIDATION GATES")
    print(f"{'='*50}")
    
    all_passed = True
    for gate_name, result in gates.items():
        status = "âœ… PASSED" if result['passed'] else "âŒ FAILED"
        print(f"{gate_name}: {status}")
        print(f"  Criterion: {result['criterion']}")
        print(f"  Actual: {result['actual']:.4f}")
        if not result['passed']:
            all_passed = False
    
    print(f"{'='*50}")
    if all_passed:
        print("ğŸŸ¢ ALL GATES PASSED - OK to proceed with deployment")
    else:
        print("ğŸ”´ GATE(S) FAILED - DO NOT DEPLOY")
    print(f"{'='*50}")
    
    return all_passed, gates


def calculate_lift_by_decile(y_true, y_pred):
    """Calculate lift by decile for Gate 2."""
    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})
    df['decile'] = pd.qcut(df['y_pred'], 10, labels=False, duplicates='drop')
    
    baseline_rate = y_true.mean()
    
    lift_df = df.groupby('decile').agg({
        'y_true': ['count', 'sum', 'mean']
    }).reset_index()
    lift_df.columns = ['decile', 'count', 'conversions', 'conv_rate']
    lift_df['lift'] = lift_df['conv_rate'] / baseline_rate
    
    # Top decile is decile 9 (0-indexed, highest scores)
    top_decile_lift = lift_df[lift_df['decile'] == lift_df['decile'].max()]['lift'].values[0]
    
    return lift_df, top_decile_lift


def calculate_shap_importance(model, X_train):
    """Calculate SHAP feature importance."""
    print("\nCalculating SHAP values (this may take a few minutes)...")
    
    # Use TreeExplainer for XGBoost
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_train.sample(min(1000, len(X_train)), random_state=42))
    
    # Feature importance from SHAP
    importance_df = pd.DataFrame({
        'feature': FEATURES_V42,
        'importance': np.abs(shap_values).mean(axis=0)
    }).sort_values('importance', ascending=False)
    
    return importance_df, shap_values, explainer


def save_model_artifacts(model, metrics, gates, importance_df, lift_df):
    """Save all model artifacts."""
    
    # 1. Save model
    model.save_model(f"{OUTPUT_DIR}/model.json")
    with open(f"{OUTPUT_DIR}/model.pkl", 'wb') as f:
        pickle.dump(model, f)
    
    # 2. Save hyperparameters
    with open(f"{OUTPUT_DIR}/hyperparameters.json", 'w') as f:
        json.dump(HYPERPARAMETERS, f, indent=2)
    
    # 3. Save training metrics
    training_metrics = {
        'model_version': MODEL_VERSION,
        'trained_at': datetime.now().isoformat(),
        'features': FEATURES_V42,
        'feature_count': len(FEATURES_V42),
        'train_auc_roc': round(metrics['train_auc'], 4),
        'test_auc_roc': round(metrics['test_auc'], 4),
        'test_auc_pr': round(metrics['test_ap'], 4),
        'overfitting_gap': round(metrics['overfitting_gap'], 4),
        'baseline_comparison': {
            'v41_test_auc': 0.620,
            'v42_test_auc': round(metrics['test_auc'], 4),
            'improvement': round((metrics['test_auc'] - 0.620) * 100, 2)
        },
        'gates_passed': all(g['passed'] for g in gates.values()),
        'gate_results': {k: {kk: str(vv) if isinstance(vv, bool) else vv for kk, vv in v.items()} for k, v in gates.items()}
    }
    with open(f"{OUTPUT_DIR}/training_metrics.json", 'w') as f:
        json.dump(training_metrics, f, indent=2)
    
    # 4. Save feature importance
    importance_df.to_csv(f"{OUTPUT_DIR}/feature_importance.csv", index=False)
    importance_df.to_csv(f"{REPORTS_DIR}/shap_importance.csv", index=False)
    
    # 5. Save lift by decile
    lift_df.to_csv(f"{REPORTS_DIR}/lift_by_decile.csv", index=False)
    
    print(f"\nâœ… Artifacts saved to {OUTPUT_DIR}/")
    print(f"âœ… Reports saved to {REPORTS_DIR}/")


def main():
    """Main training pipeline."""
    print("="*60)
    print(f"V4.2.0 MODEL TRAINING - AGE BUCKET FEATURE")
    print(f"Started: {datetime.now().isoformat()}")
    print("="*60)
    
    # Step 1: Load data
    print("\n[1/5] Loading training data...")
    df = load_training_data()
    
    # Step 2: Train model
    print("\n[2/5] Training XGBoost model...")
    model, metrics = train_model(df)
    
    # Step 3: Calculate lift by decile
    print("\n[3/5] Calculating lift by decile...")
    lift_df, top_decile_lift = calculate_lift_by_decile(
        metrics['y_test'], 
        metrics['y_test_pred']
    )
    metrics['top_decile_lift'] = top_decile_lift
    print(f"Top Decile Lift: {top_decile_lift:.2f}x")
    print(lift_df.to_string(index=False))
    
    # Step 4: Validate gates
    print("\n[4/5] Validating improvement gates...")
    all_passed, gates = validate_gates(metrics)
    
    # Add Gate 2 (top decile lift) 
    gates['G2_LIFT'] = {
        'criterion': 'Top Decile Lift >= 2.03x',
        'actual': top_decile_lift,
        'passed': top_decile_lift >= 2.03
    }
    
    if not gates['G2_LIFT']['passed']:
        print(f"âŒ G2_LIFT FAILED: {top_decile_lift:.2f}x < 2.03x")
        all_passed = False
    
    # Step 5: Calculate SHAP & save (only if gates pass)
    if all_passed:
        print("\n[5/5] Calculating SHAP importance and saving artifacts...")
        importance_df, shap_values, explainer = calculate_shap_importance(
            model, metrics['X_train']
        )
        
        # Check age feature importance (Gate 4 - warning only)
        age_importance = importance_df[importance_df['feature'] == 'age_bucket_encoded']['importance'].values[0]
        age_rank = importance_df[importance_df['feature'] == 'age_bucket_encoded'].index[0] + 1
        print(f"\nAge feature importance: {age_importance:.4f} (rank #{age_rank}/{len(FEATURES_V42)})")
        
        if age_importance <= 0:
            print("âš ï¸ WARNING: Age feature has zero or negative importance - may not be useful")
        
        save_model_artifacts(model, metrics, gates, importance_df, lift_df)
        
        print("\n" + "="*60)
        print("ğŸŸ¢ V4.2.0 TRAINING COMPLETE - READY FOR DEPLOYMENT")
        print("="*60)
        
        return True, metrics, gates
    else:
        print("\n" + "="*60)
        print("ğŸ”´ V4.2.0 TRAINING COMPLETE - DO NOT DEPLOY")
        print("Age feature does not improve model performance.")
        print("Recommendation: Keep V4.1.0 in production.")
        print("="*60)
        
        return False, metrics, gates


if __name__ == "__main__":
    success, metrics, gates = main()
```

---

## Phase 3: Validation & Decision

### Step 3.1: Run Training and Evaluate

Execute the training script and record results:

```bash
cd C:\Users\russe\Documents\lead_scoring_production
python v4/training/train_v42_age_feature.py
```

### Step 3.2: Validation Gate Checklist

After training completes, fill in this checklist:

| Gate | Criterion | V4.1.0 | V4.2.0 | Passed? |
|------|-----------|--------|--------|---------|
| **G1** | Test AUC-ROC â‰¥ 0.620 | 0.620 | [FILL] | [YES/NO] |
| **G2** | Top Decile Lift â‰¥ 2.03x | 2.03x | [FILL] | [YES/NO] |
| **G3** | Overfitting Gap < 0.15 | 0.075 | [FILL] | [YES/NO] |
| **G4** | Age importance > 0 | N/A | [FILL] | [YES/NO] |

### Step 3.3: GO / NO-GO Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DECISION POINT                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  IF all gates PASSED:                                       â”‚
â”‚     â†’ Proceed to Phase 4 (Deployment)                       â”‚
â”‚     â†’ Update all documentation                              â”‚
â”‚                                                             â”‚
â”‚  IF any gate FAILED:                                        â”‚
â”‚     â†’ STOP HERE                                             â”‚
â”‚     â†’ DO NOT deploy V4.2.0                                  â”‚
â”‚     â†’ Keep V4.1.0 in production                             â”‚
â”‚     â†’ Document findings for future reference                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 4: Deployment (Only If Gates Pass)

### Step 4.1: Update Prospect Scores Table

```sql
-- ============================================================================
-- V4.2.0: UPDATE PROSPECT SCORES TABLE
-- ============================================================================
-- Only run this if all validation gates passed!
-- ============================================================================

CREATE OR REPLACE TABLE `savvy-gtm-analytics.ml_features.v4_prospect_scores` AS

WITH feature_data AS (
    SELECT *
    FROM `savvy-gtm-analytics.ml_features.v4_prospect_features`
),

-- Score using new model (run from Python and write back)
-- This is a placeholder - actual scoring done via Python inference script

scored AS (
    SELECT 
        crd,
        -- Scores will be populated by inference script
        v4_score,
        v4_percentile,
        v4_deprioritize,
        v4_upgrade_candidate,
        -- SHAP features
        shap_top1_feature,
        shap_top1_value,
        shap_top2_feature,
        shap_top2_value,
        shap_top3_feature,
        shap_top3_value,
        v4_narrative,
        -- Metadata
        'v4.2.0' as model_version,
        CURRENT_TIMESTAMP() as scored_at
    FROM feature_data
)

SELECT * FROM scored;
```

### Step 4.2: Python Inference Script Update

Update the inference script to use V4.2.0 model:

```python
# v4/inference/lead_scorer_v4.py

# Update MODEL_PATH
MODEL_PATH = "v4/models/v4.2.0/model.pkl"
MODEL_VERSION = "v4.2.0"

# Update FEATURES list
FEATURES = [
    'tenure_months',
    'tenure_bucket_encoded',
    'experience_years',
    'mobility_3yr',
    'mobility_tier_encoded',
    'firm_net_change_12mo',
    'firm_stability_tier_encoded',
    'firm_rep_count_at_contact',
    'is_wirehouse',
    'is_broker_protocol',
    'has_email',
    'has_linkedin',
    'is_recent_mover',
    'days_since_last_move',
    'firm_departures_corrected',
    'bleeding_velocity_encoded',
    'is_independent_ria',
    'is_ia_rep_type',
    'is_dual_registered',
    'short_tenure_x_high_mobility',
    'mobility_x_heavy_bleeding',
    'age_bucket_encoded'  # NEW V4.2.0
]
```

---

## Phase 5: Documentation Updates

### Step 5.1: Update `v4/models/registry.json`

Add V4.2.0 entry:

```json
{
  "models": [
    {
      "version": "v4.2.0",
      "status": "production",
      "deployed_date": "2026-01-XX",
      "features": 23,
      "changes_from_v41": [
        "Added age_bucket_encoded feature (23rd feature)",
        "Age encoding: UNDER_35=0, 35_49=1, 50_64=2, 65_69=3, 70_PLUS=4",
        "Based on age_analysis_results.md (January 7, 2026)"
      ],
      "performance": {
        "test_auc_roc": "[FILL]",
        "top_decile_lift": "[FILL]",
        "vs_v41_improvement": "[FILL]%"
      },
      "artifacts": {
        "model": "v4/models/v4.2.0/model.pkl",
        "hyperparameters": "v4/models/v4.2.0/hyperparameters.json",
        "training_metrics": "v4/models/v4.2.0/training_metrics.json"
      }
    },
    {
      "version": "v4.1.0_r3",
      "status": "deprecated",
      "deployed_date": "2025-12-30",
      "deprecated_date": "2026-01-XX",
      "features": 22,
      "performance": {
        "test_auc_roc": 0.620,
        "top_decile_lift": 2.03
      }
    }
  ]
}
```

### Step 5.2: Update `v4/VERSION_4_MODEL_REPORT.md`

Add new section:

```markdown
---

## V4.2.0: Age Feature Addition (January 2026)

### Background

Analysis of historical conversion data revealed that age provides a unique signal independent of existing features:
- Correlation between age and `experience_years`: **0.072** (not redundant)
- V4.1.0 doesn't fully capture age effect (65+ converts at 1.22% vs 3.46% for under 50)
- Even in V4.1.0 top decile, Over 65 converts worse (3.58% vs 4.83%)

### Change: Added `age_bucket_encoded` Feature

| Bucket | Value | Age Range | Historical Conv Rate |
|--------|-------|-----------|---------------------|
| UNDER_35 | 0 | 18-34 | 4.06% |
| 35_49 | 1 | 35-49 | 3.82% |
| 50_64 | 2 | 50-64 | 3.56% |
| 65_69 | 3 | 65-69 | 2.97% |
| 70_PLUS | 4 | 70+ | 1.48% |

### Performance Comparison

| Metric | V4.1.0 R3 | V4.2.0 | Change |
|--------|-----------|--------|--------|
| Features | 22 | 23 | +1 |
| Test AUC-ROC | 0.620 | [FILL] | [FILL] |
| Test AUC-PR | 0.070 | [FILL] | [FILL] |
| Top Decile Lift | 2.03x | [FILL] | [FILL] |
| Overfitting Gap | 0.075 | [FILL] | [FILL] |

### Validation Gates

| Gate | Criterion | Result | Status |
|------|-----------|--------|--------|
| G1 | Test AUC â‰¥ 0.620 | [FILL] | [PASS/FAIL] |
| G2 | Top Decile Lift â‰¥ 2.03x | [FILL] | [PASS/FAIL] |
| G3 | Overfitting Gap < 0.15 | [FILL] | [PASS/FAIL] |
| G4 | Age Importance > 0 | [FILL] | [PASS/WARN] |

### Age Feature Importance

Rank: #[FILL] of 23 features
SHAP Importance: [FILL]

### Files Updated

- `ml_features.v4_prospect_features` - Added age_bucket_encoded
- `ml_features.v4_features_pit_v42` - Training data with age
- `ml_features.v4_prospect_scores` - Rescored with V4.2.0
- `v4/models/v4.2.0/` - New model artifacts
- `v4/models/registry.json` - Version registry
```

### Step 5.3: Create `v4/reports/v4.2/V4.2_Final_Summary.md`

```markdown
# V4.2.0 Final Summary Report

**Model Version**: V4.2.0  
**Deployment Date**: 2026-01-XX  
**Status**: [DEPLOYED / NOT DEPLOYED]

---

## Executive Summary

V4.2.0 adds `age_bucket_encoded` as the 23rd feature based on age analysis showing age provides unique signal (r=0.072 with experience_years).

### Performance

| Metric | V4.1.0 R3 | V4.2.0 | Improvement |
|--------|-----------|--------|-------------|
| Test AUC-ROC | 0.620 | [FILL] | [FILL] |
| Test AUC-PR | 0.070 | [FILL] | [FILL] |
| Top Decile Lift | 2.03x | [FILL] | [FILL] |

### Validation Gates

All gates: [PASSED / FAILED]

| Gate | Status |
|------|--------|
| G1: AUC â‰¥ 0.620 | [âœ…/âŒ] |
| G2: Lift â‰¥ 2.03x | [âœ…/âŒ] |
| G3: Overfit < 0.15 | [âœ…/âŒ] |
| G4: Age Imp > 0 | [âœ…/âš ï¸] |

### New Feature

**age_bucket_encoded**:
- Encoding: UNDER_35=0, 35_49=1, 50_64=2, 65_69=3, 70_PLUS=4
- SHAP Importance: [FILL]
- Rank: #[FILL] of 23

### Conclusion

[IF DEPLOYED]: V4.2.0 improves upon V4.1.0 by incorporating age signal. Recommended for production use.

[IF NOT DEPLOYED]: Age feature did not improve model performance. V4.1.0 remains in production.
```

### Step 5.4: Create `v4/reports/v4.2/model_validation_report.md`

```markdown
# V4.2.0 Model Validation Report

**Generated**: [TIMESTAMP]
**Model**: V4.2.0 (23 features)
**Baseline**: V4.1.0 R3 (22 features)

---

## 1. Training Summary

| Metric | Value |
|--------|-------|
| Training Samples | [FILL] |
| Test Samples | [FILL] |
| Positive Rate (Train) | [FILL]% |
| Positive Rate (Test) | [FILL]% |
| Features | 23 |

## 2. Performance Metrics

### 2.1 AUC Comparison

| Dataset | V4.1.0 | V4.2.0 | Delta |
|---------|--------|--------|-------|
| Train | 0.695 | [FILL] | [FILL] |
| Test | 0.620 | [FILL] | [FILL] |

### 2.2 Lift by Decile

| Decile | V4.2.0 Conv Rate | V4.2.0 Lift | V4.1.0 Lift |
|--------|------------------|-------------|-------------|
| 10 (Top) | [FILL] | [FILL] | 2.03x |
| 9 | [FILL] | [FILL] | |
| 8 | [FILL] | [FILL] | |
| 7 | [FILL] | [FILL] | |
| 6 | [FILL] | [FILL] | |
| 5 | [FILL] | [FILL] | |
| 4 | [FILL] | [FILL] | |
| 3 | [FILL] | [FILL] | |
| 2 | [FILL] | [FILL] | |
| 1 (Bot) | [FILL] | [FILL] | 0.25x |

## 3. Feature Importance (SHAP)

| Rank | Feature | Importance |
|------|---------|------------|
| 1 | [FILL] | [FILL] |
| 2 | [FILL] | [FILL] |
| 3 | [FILL] | [FILL] |
| ... | ... | ... |
| [X] | age_bucket_encoded | [FILL] |

## 4. Validation Gates

| Gate | Criterion | Actual | Status |
|------|-----------|--------|--------|
| G1 | Test AUC â‰¥ 0.620 | [FILL] | [âœ…/âŒ] |
| G2 | Top Decile â‰¥ 2.03x | [FILL] | [âœ…/âŒ] |
| G3 | Overfit < 0.15 | [FILL] | [âœ…/âŒ] |
| G4 | Age Imp > 0 | [FILL] | [âœ…/âš ï¸] |

## 5. Recommendation

[DEPLOY / DO NOT DEPLOY]

Rationale: [FILL]
```

---

## Phase 6: Rollback Plan

If V4.2.0 causes issues in production:

```sql
-- ROLLBACK: Restore V4.1.0 scores
CREATE OR REPLACE TABLE `savvy-gtm-analytics.ml_features.v4_prospect_scores` AS
SELECT * FROM `savvy-gtm-analytics.ml_features.v4_prospect_scores_v41_backup`;
```

```python
# ROLLBACK: Restore V4.1.0 model in inference
MODEL_PATH = "v4/models/v4.1.0_r3/model.pkl"
MODEL_VERSION = "v4.1.0_r3"
# Remove age_bucket_encoded from FEATURES list
```

---

## Summary Checklist

### Before Training
- [ ] Create `v4/models/v4.2.0/` directory
- [ ] Create `v4/reports/v4.2/` directory
- [ ] Update `v4_prospect_features` view with age_bucket
- [ ] Create `v4_features_pit_v42` table
- [ ] Verify age_bucket distribution looks correct

### Training
- [ ] Run training script
- [ ] Record all metrics
- [ ] Check validation gates

### If Gates Pass
- [ ] Save model artifacts
- [ ] Generate SHAP plots
- [ ] Update `v4_prospect_scores` table
- [ ] Update `registry.json`
- [ ] Update `VERSION_4_MODEL_REPORT.md`
- [ ] Create `V4.2_Final_Summary.md`
- [ ] Create `model_validation_report.md`
- [ ] Update inference script
- [ ] Create backup of V4.1.0 scores

### If Gates Fail
- [ ] Document findings in `v4/reports/v4.2/age_feature_analysis_negative.md`
- [ ] Keep V4.1.0 in production
- [ ] No further action needed

---

## Appendix: File Locations

| File | Path | Action |
|------|------|--------|
| Training script | `v4/training/train_v42_age_feature.py` | Create |
| Model artifacts | `v4/models/v4.2.0/` | Create dir |
| Reports | `v4/reports/v4.2/` | Create dir |
| Registry | `v4/models/registry.json` | Update |
| Model report | `v4/VERSION_4_MODEL_REPORT.md` | Update |
| Inference | `v4/inference/lead_scorer_v4.py` | Update |
| Features view | `ml_features.v4_prospect_features` | Update |
| Training data | `ml_features.v4_features_pit_v42` | Create |
| Scores table | `ml_features.v4_prospect_scores` | Update |
